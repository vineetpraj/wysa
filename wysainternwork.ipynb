{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**WYSA NLP Intern Role Assignment**\n",
        "\n",
        "\n",
        "Greetings!! My name is Vineet and this is my assignement for the NLP Engineer Intern role at WYSA. Since I am a budding in the field of computational lingusitics and NLP I have taken referred a bunch of sources(classcentral, Medium, Github etc.) to write the code for the proposed problem statement.\n",
        "\n",
        "I hope you would like my work."
      ],
      "metadata": {
        "id": "EzWpfDYCQ4L7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For performing operations on the dataset, I have first tried to create a simplistic label column in the dataset:0---1---2(Negatice-Neutral-Positive).\n",
        "\n",
        "To do this I have used pandas library to make the apt changes."
      ],
      "metadata": {
        "id": "qwCWqv8qzyuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = '/content/dataset-init.csv'\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "target_column = 'is_there_an_emotion_directed_at_a_brand_or_product'\n",
        "\n",
        "data[target_column] = data[target_column].replace({\n",
        "    'Negative emotion': '0',\n",
        "    'Positive emotion': '2',\n",
        "    'No emotion toward brand or product': '1'\n",
        "})\n",
        "\n",
        "\n",
        "v1_path = '/content/v1.csv'\n",
        "data.to_csv(v1_path, index=False)\n",
        "\n",
        "print(\"v1 of the dataset with new labels has been saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ8x4aYJzyLD",
        "outputId": "257deb70-a33a-48bd-f65d-1b050dd8ac7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v1 of the dataset with new labels has been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing & EDA for NLP**\n",
        "\n",
        "*Data - Cleaning*\n",
        "- **Tokenization** : involves breaking down text into smaller, manageable units called tokens. These small units act as units for feature extraction.\n",
        "\n",
        "- **StopWords** : These are recurring words that do not add contextual knowledge to the text, thus are irrelevant and needs to be removed.\n",
        "\n",
        "- **Casing** : Given the collation of the data, a standardized casing of characters is used. For instance, I have used lowercase.\n",
        "\n",
        "- **Contractions** : Colloquially contractions(such as aren't,won't) are used due to which negation on the text is often subdued to prevent this it is replaced with standard lexicons.\n",
        "\n",
        "- **Lemmatization** : Nomalization of disambiguation to base form of language.\n"
      ],
      "metadata": {
        "id": "RsX_9csQ3LbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "v1 = '/content/v1.csv'\n",
        "v2 = '/content/v2.csv'\n",
        "data = pd.read_csv(v1)\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text) # Removing special characters and punctuation\n",
        "        tokens = word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "        return ' '.join(tokens)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "\n",
        "text_columns = data.columns[:2] #Applied to the two columns except the label\n",
        "data[text_columns] = data[text_columns].applymap(preprocess_text)\n",
        "\n",
        "\n",
        "data.to_csv(v2, index=False)\n",
        "\n",
        "print(\"Preprocessed data saved v2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQCII4Dz7u7Y",
        "outputId": "1ed64bd6-4446-45fd-f018-690c0b679d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "<ipython-input-13-d4ea1ab63dd0>:35: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  data[text_columns] = data[text_columns].applymap(preprocess_text)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data saved v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training**\n",
        "\n",
        "Here is how I tackled the main problem. As per the EDA I realised that for every neutral sentiment no product or service was targeted. Thus I trained a model first for sentiment analysis only. Then, if the sentiment is either 0 or 2 then it would be evaluated by another model for determining the the targeted product/service. I split the dataset into training and validation set for the same. I decided on using SVM based on the scatter plots I generated. The accuracy score for the first model is descent but I do not understand the flaw with the second model.(it shows 100% accuracy) ig I made some errors or the model is over-fitting."
      ],
      "metadata": {
        "id": "kB1VtAj_E39N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, f1_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "v2 = '/content/v2.csv'\n",
        "data = pd.read_csv(v2)\n",
        "\n",
        "X = data[data.columns[1]]\n",
        "y_sentiment = data[data.columns[2]]  # label column\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_sentiment, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handling NaN values\n",
        "X_train = X_train.fillna('')\n",
        "X_val = X_val.fillna('')\n",
        "\n",
        "# Vectorizing the text data\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf.transform(X_val)\n",
        "\n",
        "# creating classification  model\n",
        "model_sentiment = SVC(max_iter=1000)\n",
        "model_sentiment.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_sentiment = model_sentiment.predict(X_val_tfidf)\n",
        "\n",
        "\n",
        "print(\"Classification Report for Sentiment Analysis:\")\n",
        "print(classification_report(y_val, y_pred_sentiment))\n",
        "print(\"F1 Score:\", f1_score(y_val, y_pred_sentiment, average='weighted'))\n",
        "print(\"Recall Score:\", recall_score(y_val, y_pred_sentiment, average='weighted'))\n",
        "print(\"Accuracy Score:\", accuracy_score(y_val, y_pred_sentiment))\n",
        "\n",
        "\n",
        "# Creating a second model to predict product/service(0&2)\n",
        "X_product = X[y_sentiment != 1]  # removing the ones with neutral sentiment\n",
        "y_product = data[data.columns[1]][y_sentiment != 1]  # product/service labels\n",
        "\n",
        "X_train_prod, X_val_prod, y_train_prod, y_val_prod = train_test_split(X_product, y_product, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handling NaN values\n",
        "X_train_prod = X_train_prod.fillna('')\n",
        "X_val_prod = X_val_prod.fillna('')\n",
        "y_train_prod = y_train_prod.fillna('')\n",
        "y_val_prod = y_val_prod.fillna('')\n",
        "\n",
        "X_train_prod_tfidf = tfidf.fit_transform(X_train_prod)\n",
        "X_val_prod_tfidf = tfidf.transform(X_val_prod)\n",
        "\n",
        "model_product = SVC(max_iter=1000)\n",
        "model_product.fit(X_train_prod_tfidf, y_train_prod)\n",
        "\n",
        "\n",
        "y_pred_product = model_product.predict(X_val_prod_tfidf)\n",
        "\n",
        "print(\"Classification Report for Product/Service Prediction:\")\n",
        "print(classification_report(y_val_prod, y_pred_product))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBTELKwa3G2R",
        "outputId": "1056f902-bc3e-4e48-df7a-1e1c1b18853b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Sentiment Analysis:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       103\n",
            "           1       1.00      0.98      0.99      1059\n",
            "           2       0.82      1.00      0.90       556\n",
            "\n",
            "    accuracy                           0.93      1718\n",
            "   macro avg       0.61      0.66      0.63      1718\n",
            "weighted avg       0.88      0.93      0.90      1718\n",
            "\n",
            "F1 Score: 0.901938167828826\n",
            "Recall Score: 0.9284051222351571\n",
            "Accuracy Score: 0.9284051222351571\n",
            "Classification Report for Product/Service Prediction:\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "                             1.00      1.00      1.00      1039\n",
            "               android       1.00      1.00      1.00        18\n",
            "           android app       1.00      1.00      1.00        17\n",
            "                 apple       1.00      1.00      1.00       134\n",
            " apple product service       1.00      1.00      1.00         7\n",
            "                google       1.00      1.00      1.00        76\n",
            "google product service       1.00      1.00      1.00        68\n",
            "                  ipad       1.00      1.00      1.00       194\n",
            "       ipad iphone app       1.00      1.00      1.00       103\n",
            "                iphone       1.00      1.00      1.00        62\n",
            "\n",
            "              accuracy                           1.00      1718\n",
            "             macro avg       1.00      1.00      1.00      1718\n",
            "          weighted avg       1.00      1.00      1.00      1718\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing from the test data\n",
        "input_text = input(\"Enter a review text: \")\n",
        "preprocessed_text = preprocess_text(input_text)\n",
        "input_tfidf = tfidf.transform([preprocessed_text])\n",
        "sentiment_prediction = model_sentiment.predict(input_tfidf)\n",
        "print(\"Predicted Sentiment:\", \"Positive\" if sentiment_prediction[0] == '2' else (\"Negative\" if sentiment_prediction[0] == '0' else \"Neutral\"))\n",
        "if sentiment_prediction[0] in ['0', '2']:\n",
        "    product_prediction = model_product.predict(input_tfidf)\n",
        "    print(\"Targeted Product/Service:\", product_prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvC0dSWTHrrJ",
        "outputId": "fbd5e68a-8518-47c0-95fe-0799ef6dbcac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a review text: GSD&amp;M Google #sxsw party with @mention is blowing my mind. So much amazing food, drink, music, &amp; photo booth fun!\n",
            "Predicted Sentiment: Positive\n",
            "Targeted Product/Service: google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction for the Test Data**"
      ],
      "metadata": {
        "id": "js8ini3EHq7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    test = '/content/dataset_test.csv'\n",
        "    test_data = pd.read_csv(test)\n",
        "    text_columns = test_data.columns[:0]\n",
        "    test_data[text_columns] = test_data[text_columns].applymap(preprocess_text)\n",
        "    X_test_tfidf = tfidf.transform(test_data[test_data.columns[0]])\n",
        "\n",
        "    sentiment_predictions = model_sentiment.predict(X_test_tfidf)\n",
        "\n",
        "    product_predictions = []\n",
        "    for index, sentiment_prediction in enumerate(sentiment_predictions):\n",
        "        if sentiment_prediction in ['0', '2']:\n",
        "            product_predictions.append(model_product.predict(X_test_tfidf[index])[0])\n",
        "        else:\n",
        "            product_predictions.append(\"Not Applicable\")\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Sentiment': sentiment_predictions,\n",
        "        'Product/Service': product_predictions\n",
        "    })\n",
        "\n",
        "    pred = '/content/test-pred.csv'\n",
        "    predictions_df.to_csv(pred,index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyFyHGsfK7R-",
        "outputId": "735870fe-1909-4ba5-9a73-4ec643097ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-20675d662794>:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  test_data[text_columns] = test_data[text_columns].applymap(preprocess_text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZX_qTKWmTaaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}